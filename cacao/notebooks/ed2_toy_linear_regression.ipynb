{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow_probability import edward2 as ed\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_regression(n_samples=1000000, n_features=10, n_informative=7)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model(X): # (unmodeled) data\n",
    "    w = ed.Normal(loc=tf.zeros([X.shape[1]]),\n",
    "        scale=tf.ones([X.shape[1]]),\n",
    "        name=\"w\")  # parameter\n",
    "    b = ed.Normal(loc=tf.zeros([]),\n",
    "        scale=tf.ones([]), \n",
    "        name=\"b\")  # parameter\n",
    "    y = ed.Normal(loc=tf.tensordot(X, w, axes=1) + b, scale=1.0, \n",
    "        name=\"y\")  # (modeled) data\n",
    "    return y   \n",
    "\n",
    "def variational_model(qw_mean, qw_stddv, qb_mean, qb_stddv):\n",
    "    qw = ed.Normal(loc=qw_mean, scale=qw_stddv, name=\"qw\")\n",
    "    qb = ed.Normal(loc=qb_mean, scale=qb_stddv, name=\"qb\")          \n",
    "    return qw, qb\n",
    "\n",
    "def fit():\n",
    "    \n",
    "    x_tensor = tf.convert_to_tensor(x_train, tf.float32)\n",
    "    y_tensor = tf.convert_to_tensor(y_train, tf.float32)\n",
    "    \n",
    "    # make dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_tensor, y_tensor))\n",
    "    shuffle = dataset.shuffle(1000)\n",
    "    batches = shuffle.repeat().batch(50)\n",
    "    iterator = batches.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "\n",
    "    log_q = ed.make_log_joint_fn(variational_model)\n",
    "\n",
    "    def target_q(qw, qb):\n",
    "        return log_q(qw_mean=qw_mean, qw_stddv=qw_stddv, qb_mean=qb_mean, qb_stddv=qb_stddv, qw=qw, qb=qb)\n",
    "\n",
    "    qw_mean = tf.Variable(tf.zeros([int(features.shape[1])]), dtype=tf.float32)\n",
    "    qb_mean = tf.Variable(tf.zeros([]), dtype=tf.float32)\n",
    "    qw_stddv = tf.nn.softplus(tf.Variable(tf.ones([int(features.shape[1])]), dtype=tf.float32))\n",
    "    qb_stddv = tf.nn.softplus(tf.Variable(tf.ones([]), dtype=tf.float32))\n",
    "\n",
    "    qw, qb = variational_model(qw_mean=qw_mean, qw_stddv=qw_stddv,\n",
    "                            qb_mean=qb_mean, qb_stddv=qb_stddv)\n",
    "\n",
    "    log_joint = ed.make_log_joint_fn(log_model)\n",
    "\n",
    "    def target(qw, qb):\n",
    "        \"\"\"Unnormalized target density as a function of the parameters.\"\"\"\n",
    "        return log_joint(w=qw, b=qb, X=features, y=labels)\n",
    "\n",
    "    energy = target(qw, qb) \n",
    "    entropy = -target_q(qw, qb) / 50\n",
    "    elbo = energy + entropy\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = .001)\n",
    "    train = optimizer.minimize(-elbo)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    t = []\n",
    "    weights_dict = {}\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for step in range(200):\n",
    "            _ = sess.run([train])\n",
    "            if step % 100 == 0:\n",
    "                t.append(sess.run([elbo]))\n",
    "        \n",
    "        weights_dict['w'], weights_dict['b'] = sess.run([qw.distribution.sample(1000), qb.distribution.sample(1000)])\n",
    "    \n",
    "    return weights_dict['w'], weights_dict['b']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "\n",
    "    features, labels = tf.convert_to_tensor(x_test, tf.float32), tf.convert_to_tensor(y_test, tf.float32)\n",
    "\n",
    "    w_ = tf.reduce_mean(qw_, 0)\n",
    "    b_ = tf.reduce_mean(qb_, 0)\n",
    "\n",
    "    pred_tensor = tf.cast((tf.tensordot(features, w_, axes=1) + b_), tf.float32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((labels, pred_tensor))\n",
    "    batches = dataset.repeat().batch(1000)\n",
    "    iterator = batches.make_one_shot_iterator()\n",
    "    labels, predictions = iterator.get_next()\n",
    "\n",
    "    rmse, rmse_update_op = tf.metrics.root_mean_squared_error(tf.cast(labels,tf.float32), predictions)\n",
    "\n",
    "    n_batches = int(int(features.shape[0]) / 50)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        for _i in range(n_batches):\n",
    "            rmse_ = sess.run([rmse_update_op])\n",
    "\n",
    "        metrics = {}\n",
    "        metrics[\"rmse\"] = sess.run([rmse])\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw_, qb_ = fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
